{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a261f254966e46c7adbc39d905a5f940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07e61f6d64714e5db02f3a10d041b9fa",
              "IPY_MODEL_e586ea6724b74bc18ea65a72c875e91a",
              "IPY_MODEL_2a110aa990914bf1af68cb59f6fd7696"
            ],
            "layout": "IPY_MODEL_00c9905244fe4464876d0663cd7edf20"
          }
        },
        "07e61f6d64714e5db02f3a10d041b9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14d6e9135e254976b262af99abb28ab9",
            "placeholder": "​",
            "style": "IPY_MODEL_5b7c74468f2a47cdb3069be58c4a2e57",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e586ea6724b74bc18ea65a72c875e91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b649f75e7c944864813c71ef9fef9ca3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b3d34584b294fac8b3f2c7c9769dd5d",
            "value": 2
          }
        },
        "2a110aa990914bf1af68cb59f6fd7696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_854c7bac4d87446da4fdb0845df96141",
            "placeholder": "​",
            "style": "IPY_MODEL_f3ef7d49394d4443b6fae21b6d213256",
            "value": " 2/2 [00:24&lt;00:00, 10.79s/it]"
          }
        },
        "00c9905244fe4464876d0663cd7edf20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14d6e9135e254976b262af99abb28ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7c74468f2a47cdb3069be58c4a2e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b649f75e7c944864813c71ef9fef9ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3d34584b294fac8b3f2c7c9769dd5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "854c7bac4d87446da4fdb0845df96141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3ef7d49394d4443b6fae21b6d213256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U xformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U einops\n",
        "!pip install -q -U nvidia-ml-py3\n",
        "!pip install -q -U huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvMRBUAw6RNO",
        "outputId": "8e7c8cce-6017-4bd5-b893-7eb5f009900f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from pynvml import *\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
        "import time, torch"
      ],
      "metadata": {
        "id": "RFAMCeCHl_fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "5tXHwxLol5s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_eos_token=True, use_fast=True, max_length=250)\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWFDrw1ul9Lg",
        "outputId": "ced71b47-cf63-4c01-95f2-c82d2a0d0f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          base_model_id, trust_remote_code=True, quantization_config=bnb_config, device_map = {\"\": 0}, torch_dtype=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "NSrrVpOU6Rtl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a261f254966e46c7adbc39d905a5f940",
            "07e61f6d64714e5db02f3a10d041b9fa",
            "e586ea6724b74bc18ea65a72c875e91a",
            "2a110aa990914bf1af68cb59f6fd7696",
            "00c9905244fe4464876d0663cd7edf20",
            "14d6e9135e254976b262af99abb28ab9",
            "5b7c74468f2a47cdb3069be58c4a2e57",
            "b649f75e7c944864813c71ef9fef9ca3",
            "4b3d34584b294fac8b3f2c7c9769dd5d",
            "854c7bac4d87446da4fdb0845df96141",
            "f3ef7d49394d4443b6fae21b6d213256"
          ]
        },
        "outputId": "11cbdb1e-0237-4e57-c066-37fa2fdabd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a261f254966e46c7adbc39d905a5f940"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnOC75fLkpug",
        "outputId": "268abfa0-d6a5-4abe-c95e-63ec0c4aa47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PhiForCausalLM(\n",
            "  (model): PhiModel(\n",
            "    (embed_tokens): Embedding(51200, 2560)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x PhiDecoderLayer(\n",
            "        (self_attn): PhiAttention(\n",
            "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "          (rotary_emb): PhiRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): PhiMLP(\n",
            "          (activation_fn): NewGELUActivation()\n",
            "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
            "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
            "        )\n",
            "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        r=16,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules= [\"q_proj\", \"k_proj\"])"
      ],
      "metadata": {
        "id": "mg14ctLb9O_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "PutTg0WsmwKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAIpGWBpunlb",
        "outputId": "c7f88f38-27a6-4f3e-9720-9f8bb8deefd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): PeftModelForCausalLM(\n",
            "      (base_model): LoraModel(\n",
            "        (model): PhiForCausalLM(\n",
            "          (model): PhiModel(\n",
            "            (embed_tokens): Embedding(51200, 2560)\n",
            "            (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (layers): ModuleList(\n",
            "              (0-31): 32 x PhiDecoderLayer(\n",
            "                (self_attn): PhiAttention(\n",
            "                  (q_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=2560, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=2560, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (k_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=2560, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=2560, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "                  (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
            "                  (rotary_emb): PhiRotaryEmbedding()\n",
            "                )\n",
            "                (mlp): PhiMLP(\n",
            "                  (activation_fn): NewGELUActivation()\n",
            "                  (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
            "                  (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
            "                )\n",
            "                (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"csv\", data_files=\"prompts.csv\")"
      ],
      "metadata": {
        "id": "RGQkzOq6SVrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(sample):\n",
        "  system_prompt_template = \"\"\"<s>\n",
        "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
        "### Instruction :<<user_question>>\n",
        "### Database Schema:\n",
        "<<database_schema>>\n",
        "### Response:\n",
        "<<user_response>>\n",
        "</s>\n",
        "\"\"\"\n",
        "  user_message = sample['question']\n",
        "  user_response = sample['answer']\n",
        "  database_schema = sample['context']\n",
        "  prompt_template = system_prompt_template.replace(\"<<user_question>>\",f\"{user_message}\").replace(\"<<user_response>>\",f\"{user_response}\").replace(\"<<database_schema>>\",f\"{database_schema} \")\n",
        "\n",
        "  return {\"inputs\":prompt_template}\n",
        "\n",
        "#\n",
        "instruct_tune_dataset = dataset.map(create_prompt)\n",
        "print(instruct_tune_dataset)"
      ],
      "metadata": {
        "id": "0NLOJWuY82cJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c29b85a-1cba-4ea0-fc85-dcb61fd88698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer', 'context', 'inputs'],\n",
            "        num_rows: 113\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"phi2-results2\",\n",
        "        save_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=12,\n",
        "        log_level=\"debug\",\n",
        "        save_steps=100,\n",
        "        logging_steps=25,\n",
        "        learning_rate=1e-4,\n",
        "        eval_steps=50,\n",
        "        optim='paged_adamw_8bit',\n",
        "        fp16=True,\n",
        "        num_train_epochs=1,\n",
        "        max_steps=200,\n",
        "        warmup_steps=10,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,)"
      ],
      "metadata": {
        "id": "iazdUVVbTIA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = instruct_tune_dataset.map(batched=True,remove_columns=['answer', 'question', 'context'])\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IgVt9dkq5S_",
        "outputId": "1604970a-3c0b-4086-d74d-7fc1079fc627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['inputs'],\n",
              "        num_rows: 113\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset[\"train\"],\n",
        "        #eval_dataset=dataset['test'],\n",
        "        peft_config=peft_config,\n",
        "        dataset_text_field=\"inputs\",\n",
        "        max_seq_length=1024,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_arguments,\n",
        "        packing=False\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "446dgnLerAJC",
        "outputId": "ec3c7129-074f-4d79-c0cd-2b6c07a65586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using auto half precision backend\n",
            "Currently training with a batch size of: 4\n",
            "***** Running training *****\n",
            "  Num examples = 113\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "  Gradient Accumulation steps = 12\n",
            "  Total optimization steps = 200\n",
            "  Number of trainable parameters = 5,242,880\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 1:04:40, Epoch 82/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.664400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.949700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.504600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.387400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.336200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.313400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.297600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.288100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-2\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-2/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-2/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-4\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-4/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-4/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-7\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-7/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-7/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-9\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-9/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-9/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-12\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-12/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-12/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-14\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-14/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-14/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-16\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-16/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-16/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-19\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-19/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-19/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-21\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-21/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-21/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-24\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-24/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-24/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-26\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-26/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-26/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-29\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-29/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-29/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-31\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-31/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-31/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-33\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-33/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-33/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-36\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-36/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-36/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-38\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-38/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-38/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-41\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-41/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-41/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-43\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-43/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-43/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-45\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-45/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-45/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-48\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-48/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-48/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-50\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-50/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-53\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-53/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-53/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-55\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-55/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-55/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-58\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-58/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-58/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-60\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-60/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-62\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-62/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-62/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-65\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-65/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-65/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-67\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-67/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-67/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-70\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-70/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-72\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-72/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-72/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-74\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-74/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-74/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-77\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-77/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-77/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-79\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-79/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-79/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-82\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-82/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-82/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-84\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-84/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-84/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-87\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-87/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-87/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-89\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-89/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-89/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-91\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-91/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-91/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-94\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-94/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-94/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-96\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-96/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-96/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-99\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-99/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-99/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-101\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-101/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-101/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-103\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-103/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-103/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-106\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-106/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-106/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-108\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-108/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-108/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-111\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-111/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-111/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-113\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-113/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-113/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-116\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-116/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-116/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-118\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-118/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-118/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-120\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-120/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-123\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-123/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-123/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-125\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-125/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-125/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-128\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-128/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-128/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-130\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-130/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-132\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-132/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-132/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-135\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-135/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-135/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-137\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-137/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-137/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-140\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-140/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-142\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-142/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-142/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-145\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-145/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-145/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-147\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-147/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-147/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-149\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-149/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-149/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-152\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-152/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-152/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-154\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-154/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-154/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-157\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-157/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-157/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-159\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-159/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-159/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-161\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-161/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-161/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-164\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-164/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-164/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-166\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-166/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-166/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-169\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-169/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-169/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-171\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-171/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-171/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-174\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-174/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-174/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-176\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-176/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-176/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-178\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-178/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-178/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-181\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-181/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-181/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-183\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-183/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-183/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-186\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-186/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-186/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-188\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-188/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-188/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-190\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-190/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-190/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-193\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-193/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-193/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-195\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-195/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-195/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-198\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-198/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-198/special_tokens_map.json\n",
            "Saving model checkpoint to phi2-results2/tmp-checkpoint-200\n",
            "tokenizer config file saved in phi2-results2/tmp-checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/tmp-checkpoint-200/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=0.5926899790763855, metrics={'train_runtime': 3900.2421, 'train_samples_per_second': 2.461, 'train_steps_per_second': 0.051, 'total_flos': 2.805847879836672e+16, 'train_loss': 0.5926899790763855, 'epoch': 82.76})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = train_dataset['train'][0]['inputs']\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1syzUOczv3Ht",
        "outputId": "d4f1254a-24fc-47e8-fbc5-c97d3e3f9a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>\\nBelow is an instruction that describes a task.Write a response that appropriately completes the request.\\n### Instruction :List all products along with their prices.\\n### Database Schema:\\nProducts (product_id, name, price, stock_quantity) \\n### Response:\\nSELECT name, price FROM Products;\\n</s>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction :List all products along with their prices.\\n### Database Schema:\\nProducts (product_id, name, price, stock_quantity) \\n### Response:\\n\""
      ],
      "metadata": {
        "id": "1FHy9UewFTp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=200,)\n",
        "result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSWRHlFLv4NB",
        "outputId": "6d918cae-5583-4639-cd68-9c5c30131861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "### Instruction :List all products along with their prices.\n",
            "### Database Schema:\n",
            "Products (product_id, name, price, stock_quantity) \n",
            "### Response:\n",
            "SELECT name, price FROM Products;\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MrAyT3jDLSB",
        "outputId": "e11aeb04-ca4a-4403-8287-d13ad37bc8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to phi2-results2\n",
            "tokenizer config file saved in phi2-results2/tokenizer_config.json\n",
            "Special tokens file saved in phi2-results2/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = train_dataset['train'][8]['inputs']\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "wTk6CymWF5B4",
        "outputId": "a29328c0-2406-433c-d16f-ecfe323af628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<s>\\nBelow is an instruction that describes a task.Write a response that appropriately completes the request.\\n### Instruction :Show the orders placed on 13-06-2023.\\n### Database Schema:\\nOrders (order_id, customer_id, order_date) \\n### Response:\\nSELECT * FROM Orders WHERE order_date = '13-06-2023';\\n</s>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Below is an instruction that describes a task.Write a response that appropriately completes the request.\\n### Instruction :Show the orders placed on 13-06-2023.\\n### Database Schema:\\nOrders (order_id, customer_id, order_date) \\n### Response:\\n\""
      ],
      "metadata": {
        "id": "5_m5unWTHefo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=200,)\n",
        "result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBiAIFXfHmGZ",
        "outputId": "3bcff4ea-c1fb-4a91-c33d-e1f95f64ee04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
            "### Instruction :Show the orders placed on 13-06-2023.\n",
            "### Database Schema:\n",
            "Orders (order_id, customer_id, order_date) \n",
            "### Response:\n",
            "SELECT * FROM Orders WHERE order_date = '13-06-2023';\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Below is an instruction that describes a task.Write a response that appropriately completes the request.\\n### Instruction :List all products along with their prices and quantities in stock, sorted by their names alphabetically.\\n### Database Schema:\\nProducts (product_id, name, price, stock_quantity) \\n### Response:\\n\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=200,)\n",
        "result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5EmcZq1Lrvr",
        "outputId": "a567896e-86ae-4d8c-bb58-1eae37a5f08e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
            "### Instruction :List all products along with their prices and quantities in stock, sorted by their names alphabetically.\n",
            "### Database Schema:\n",
            "Products (product_id, name, price, stock_quantity) \n",
            "### Response:\n",
            "SELECT name, price, stock_quantity FROM Products ORDER BY name;\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Below is an instruction that describes a task.Write a response that appropriately completes the request.\\n### Instruction : Display the products with the highest stock quantities.\\n### Database Schema:\\n Products (product_id, name, price, stock_quantity) \\n### Response:\\n\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=200,)\n",
        "result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PNdYbASMI9Y",
        "outputId": "fecc5eca-09c7-4abd-9aa1-d1c9c2c8cf27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
            "### Instruction : Display the products with the highest stock quantities.\n",
            "### Database Schema:\n",
            " Products (product_id, name, price, stock_quantity) \n",
            "### Response:\n",
            "SELECT * FROM Products WHERE stock_quantity = (SELECT MAX(stock_quantity) FROM Products);\n",
            "\n"
          ]
        }
      ]
    }
  ]
}